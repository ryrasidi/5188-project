{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS CODE IS TO CREATE THE TENSOR FOR THE TRAINING DATA ##\n",
    "\n",
    "changi_df = pd.read_csv('ST5188/Data/Final/Imputation/imp_train_set.csv')\n",
    "\n",
    "# Pivot to create a time series for each (x, y)\n",
    "changi_pivot = changi_df.pivot(index=['x', 'y'], columns='Date', values='Value')\n",
    "#print (changi_pivot)\n",
    "\n",
    "# Reset index to keep (x, y) as columns\n",
    "changi_pivot = changi_pivot.reset_index()\n",
    "\n",
    "# Convert time columns back into a NumPy array\n",
    "ts_changi = changi_pivot.iloc[:, 2:].values  # Ignore first two columns (x, y)\n",
    "loc_changi = changi_pivot.iloc[:, :2].values  # Store coordinates\n",
    "\n",
    "# Standardization with Z-score normalisation\n",
    "changi_mean_LST = ts_changi.mean()\n",
    "changi_std_LST = ts_changi.std()\n",
    "ts_changi = (ts_changi - changi_mean_LST) / changi_std_LST \n",
    "\n",
    "# Convert to PyTorch tensor wiith extra dimension for LST\n",
    "X_train_tensor_changi = torch.tensor(ts_changi, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "X_train_changi = X_train_tensor_changi\n",
    "y_train_changi = X_train_tensor_changi[:, 1:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS CODE IS TO DEFINE THE LSTM MODEL ##\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=3, output_size=1):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # LSTM output\n",
    "        out = self.fc(lstm_out)  # Fully connected layer for final prediction\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100, Loss: 1.0082\n",
      "Epoch 10/100, Loss: 0.9866\n",
      "Epoch 20/100, Loss: 0.9414\n",
      "Epoch 30/100, Loss: 0.9111\n",
      "Epoch 40/100, Loss: 0.9006\n",
      "Epoch 50/100, Loss: 0.8959\n",
      "Epoch 60/100, Loss: 0.8907\n",
      "Epoch 70/100, Loss: 0.8838\n",
      "Epoch 80/100, Loss: 0.8766\n",
      "Epoch 90/100, Loss: 0.8602\n"
     ]
    }
   ],
   "source": [
    "## THIS CODE IS TO TRAIN THE MODEL ##\n",
    "torch.manual_seed(5188)\n",
    "\n",
    "model = LSTMPredictor() # Model\n",
    "criterion = nn.MSELoss() # Loss function to track performance over epochs\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam optimizer (can be switched)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    predictions = model(X_train_changi)\n",
    "\n",
    "    loss = criterion(predictions[:, :-1, :], y_train_changi)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0: # Print loss every 10 epochs\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS CODE IS FOR FORECASTING##\n",
    "\n",
    "model.eval()\n",
    "torch.manual_seed(5188)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_pred_changi = model(X_train_changi)  # Forecast next steps\n",
    "    X_pred_changi= X_pred_changi[:, -12:, :]  # Extract last 12 bimonthly periods (2023-2024)\n",
    "\n",
    "# Convert predictions to a NumPy array\n",
    "X_pred_changi_np = X_pred_changi.squeeze().cpu().numpy()\n",
    "\n",
    "# **Denormalize predictions**\n",
    "X_pred_changi_np = (X_pred_changi_np * changi_std_LST) + changi_mean_LST  # Convert back to original from scaled values\n",
    "\n",
    "# Define bimonthly periods\n",
    "bimonthly_periods = [\n",
    "    \"Jan-Feb 2023\", \"Mar-Apr 2023\", \"May-Jun 2023\", \"Jul-Aug 2023\", \"Sep-Oct 2023\", \"Nov-Dec 2023\",\n",
    "    \"Jan-Feb 2024\", \"Mar-Apr 2024\", \"May-Jun 2024\", \"Jul-Aug 2024\", \"Sep-Oct 2024\", \"Nov-Dec 2024\"\n",
    "]\n",
    "\n",
    "# Create a long-format DataFrame\n",
    "changi_pred_df = pd.DataFrame({\n",
    "    \"x\": np.repeat(loc_changi[:, 0], len(bimonthly_periods)),  # Use stored locations\n",
    "    \"y\": np.repeat(loc_changi[:, 1], len(bimonthly_periods)),  \n",
    "    \"Date\": bimonthly_periods * len(loc_changi),\n",
    "    \"Predicted_LST\": X_pred_changi_np.flatten()  # Store denormalized values\n",
    "})\n",
    "\n",
    "# changi_pred_df.to_csv('changi_pred_long.csv', index=False) # Save to CSV for RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Forecast horizon  LSTM w/o dropout\n",
      "0                1          5.503483\n",
      "1                3          1.722315\n",
      "2                6          2.933474\n",
      "3                9          1.149421\n",
      "4               12          3.917198\n",
      "5          Overall          3.421274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naomi\\AppData\\Local\\Temp\\ipykernel_13664\\1001673750.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  rmse_by_horizon = filtered_df.groupby(\"Forecast horizon\").apply(\n"
     ]
    }
   ],
   "source": [
    "## THIS CODE IS TO CALCULATE RMSE ##\n",
    "\n",
    "# Load predicted and true values\n",
    "true_df = changi_pred_df\n",
    "pred_df = pd.read_csv(\"../../../Data/Final/TT Split/changi_test_long.csv\")\n",
    "\n",
    "# Merge predicted and actual values\n",
    "merged_df = true_df.merge(pred_df, on=[\"x\", \"y\", \"Date\"], suffixes=(\"_true\", \"_pred\"))\n",
    "\n",
    "# Define the mapping between date and forecast step\n",
    "date_to_horizon = {\n",
    "    \"Jan-Feb 2023\": 1,\n",
    "    \"May-Jun 2023\": 3,\n",
    "    \"Nov-Dec 2023\": 6,\n",
    "    \"May-Jun 2024\": 9,\n",
    "    \"Nov-Dec 2024\": 12\n",
    "}\n",
    "\n",
    "# Filter only relevant dates\n",
    "selected_dates = list(date_to_horizon.keys())\n",
    "filtered_df = merged_df[merged_df[\"Date\"].isin(selected_dates)].copy()\n",
    "\n",
    "# Map forecast horizon\n",
    "filtered_df[\"Forecast horizon\"] = filtered_df[\"Date\"].map(date_to_horizon)\n",
    "\n",
    "# Compute RMSE per forecast horizon\n",
    "rmse_by_horizon = filtered_df.groupby(\"Forecast horizon\").apply(\n",
    "    lambda g: np.sqrt(np.mean((g[\"Predicted_LST\"] - g[\"Value\"]) ** 2))\n",
    ").reset_index(name=\"LSTM w/o dropout\")\n",
    "\n",
    "# Compute overall RMSE\n",
    "overall_rmse = np.sqrt(np.mean((filtered_df[\"Predicted_LST\"] - filtered_df[\"Value\"]) ** 2))\n",
    "overall_row = pd.DataFrame({\n",
    "    \"Forecast horizon\": [\"Overall\"],\n",
    "    \"LSTM w/o dropout\": [overall_rmse]\n",
    "})\n",
    "\n",
    "# Combine RMSEs\n",
    "final_rmse_df = pd.concat([rmse_by_horizon, overall_row], ignore_index=True)\n",
    "\n",
    "# Save the table\n",
    "final_rmse_df.to_csv(\"rmse_lstm_no_dropout.csv\", index=False)\n",
    "\n",
    "# Show the result\n",
    "print(final_rmse_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=3, output_size=1, dropout=0.2):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,  # dropout only works if num_layers > 1\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/200, Loss: 1.0084\n",
      "Epoch 10/200, Loss: 0.9869\n",
      "Epoch 20/200, Loss: 0.9423\n",
      "Epoch 30/200, Loss: 0.9128\n",
      "Epoch 40/200, Loss: 0.9030\n",
      "Epoch 50/200, Loss: 0.8990\n",
      "Epoch 60/200, Loss: 0.8946\n",
      "Epoch 70/200, Loss: 0.8892\n",
      "Epoch 80/200, Loss: 0.8835\n",
      "Epoch 90/200, Loss: 0.8752\n",
      "Epoch 100/200, Loss: 0.8544\n",
      "Epoch 110/200, Loss: 0.8375\n",
      "Epoch 120/200, Loss: 0.8203\n",
      "Epoch 130/200, Loss: 0.8025\n",
      "Epoch 140/200, Loss: 0.7769\n",
      "Epoch 150/200, Loss: 0.7555\n",
      "Epoch 160/200, Loss: 0.7349\n",
      "Epoch 170/200, Loss: 0.7215\n",
      "Epoch 180/200, Loss: 0.7051\n",
      "Epoch 190/200, Loss: 0.6828\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "torch.manual_seed(5188)\n",
    "model = LSTMPredictor()\n",
    "\n",
    "# Define loss function and optimizer with weight decay\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)  # Added weight decay\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 150\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    predictions = model(X_train_changi)\n",
    "    predictions = predictions[:, :-1, :]\n",
    "\n",
    "    loss = criterion(predictions, y_train_changi)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting\n",
    "model.eval()\n",
    "torch.manual_seed(5188)\n",
    "with torch.no_grad():\n",
    "    X_pred_changi = model(X_train_changi)\n",
    "    X_pred_changi = X_pred_changi[:, -12:, :]\n",
    "\n",
    "# Denormalize predictions\n",
    "X_pred_changi_np = (X_pred_changi.squeeze().cpu().numpy() * changi_std_LST) + changi_mean_LST\n",
    "\n",
    "# Define bimonthly periods\n",
    "bimonthly_periods = [\n",
    "    \"Jan-Feb 2023\", \"Mar-Apr 2023\", \"May-Jun 2023\", \"Jul-Aug 2023\", \"Sep-Oct 2023\", \"Nov-Dec 2023\",\n",
    "    \"Jan-Feb 2024\", \"Mar-Apr 2024\", \"May-Jun 2024\", \"Jul-Aug 2024\", \"Sep-Oct 2024\", \"Nov-Dec 2024\"\n",
    "]\n",
    "\n",
    "# Create DataFrame for predictions\n",
    "changi_pred_df = pd.DataFrame({\n",
    "    \"x\": np.repeat(loc_changi[:, 0], len(bimonthly_periods)),\n",
    "    \"y\": np.repeat(loc_changi[:, 1], len(bimonthly_periods)),  \n",
    "    \"Date\": bimonthly_periods * len(loc_changi),\n",
    "    \"Predicted_LST\": X_pred_changi_np.flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Forecast horizon  LSTM w/o dropout\n",
      "0                1          5.650649\n",
      "1                3          1.419931\n",
      "2                6          2.342539\n",
      "3                9          1.214700\n",
      "4               12          5.069484\n",
      "5          Overall          3.649896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naomi\\AppData\\Local\\Temp\\ipykernel_13664\\98453864.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  rmse_by_horizon = filtered_df.groupby(\"Forecast horizon\").apply(\n"
     ]
    }
   ],
   "source": [
    "## THIS CODE IS TO CALCULATE RMSE ##\n",
    "\n",
    "# Load predicted and true values\n",
    "true_df = changi_pred_df\n",
    "pred_df = pd.read_csv(\"../../../Data/Final/TT Split/changi_test_long.csv\")\n",
    "\n",
    "# Merge predicted and actual values\n",
    "merged_df = true_df.merge(pred_df, on=[\"x\", \"y\", \"Date\"], suffixes=(\"_true\", \"_pred\"))\n",
    "\n",
    "# Define the mapping between date and forecast step\n",
    "date_to_horizon = {\n",
    "    \"Jan-Feb 2023\": 1,\n",
    "    \"May-Jun 2023\": 3,\n",
    "    \"Nov-Dec 2023\": 6,\n",
    "    \"May-Jun 2024\": 9,\n",
    "    \"Nov-Dec 2024\": 12\n",
    "}\n",
    "\n",
    "# Filter only relevant dates\n",
    "selected_dates = list(date_to_horizon.keys())\n",
    "filtered_df = merged_df[merged_df[\"Date\"].isin(selected_dates)].copy()\n",
    "\n",
    "# Map forecast horizon\n",
    "filtered_df[\"Forecast horizon\"] = filtered_df[\"Date\"].map(date_to_horizon)\n",
    "\n",
    "# Compute RMSE per forecast horizon\n",
    "rmse_by_horizon = filtered_df.groupby(\"Forecast horizon\").apply(\n",
    "    lambda g: np.sqrt(np.mean((g[\"Predicted_LST\"] - g[\"Value\"]) ** 2))\n",
    ").reset_index(name=\"LSTM w/o dropout\")\n",
    "\n",
    "# Compute overall RMSE\n",
    "overall_rmse = np.sqrt(np.mean((filtered_df[\"Predicted_LST\"] - filtered_df[\"Value\"]) ** 2))\n",
    "overall_row = pd.DataFrame({\n",
    "    \"Forecast horizon\": [\"Overall\"],\n",
    "    \"LSTM w/o dropout\": [overall_rmse]\n",
    "})\n",
    "\n",
    "# Combine RMSEs\n",
    "final_rmse_df = pd.concat([rmse_by_horizon, overall_row], ignore_index=True)\n",
    "\n",
    "# Save the table\n",
    "final_rmse_df.to_csv(\"rmse_lstm_dropout.csv\", index=False)\n",
    "\n",
    "# Show the result\n",
    "print(final_rmse_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5188",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
